{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Michael Albergo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits as bce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning to generate a samples from a gaussian or beta distribution.\n",
    "\n",
    "\n",
    "Adapted from an example working with Dr. David Lopez-Paz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, sizes, final=None):\n",
    "        super(Perceptron, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(torch.nn.Linear(sizes[i], sizes[i + 1]))\n",
    "            if i != (len(sizes) - 2):\n",
    "                layers.append(torch.nn.ReLU())\n",
    "        if final is not None:\n",
    "            layers.append(final())\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# def plot(x, y):\n",
    "#     real = x.cpu().data.numpy()\n",
    "#     print real.shape\n",
    "#     fake = y.cpu().data.numpy()\n",
    "#     print fake.shape\n",
    "#     #lims = (x.data.min() - 0.1, x.data.max() + 0.1)\n",
    "#     xlim = (-3.5, 3.5)\n",
    "#     plt.figure(figsize=(2, 2))\n",
    "#     plt.hist(real, label='real', bins=np.arange(-3.5, 3.5, 0.2))\n",
    "#     plt.hist(fake, alpha=0.25, label='fake', bins=np.arange(-3.5, 3.5, 0.2))\n",
    "#     plt.axis('on')\n",
    "#     plt.gca().axes.get_xaxis().set_visible(True)\n",
    "#     plt.gca().axes.get_yaxis().set_visible(True)\n",
    "#     plt.xlim(xlim)\n",
    "#     plt.tight_layout(0, 0, 0)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "def means_gaussian(std = 1, n_samples = 1000):\n",
    "    m = torch.Tensor(n_samples, 1)\n",
    "    return (m.normal_(0,std))\n",
    "\n",
    "def means_beta(a=0.7, b=0.7, n_samples=1000):\n",
    "    m = np.random.beta(a=0.7, b = 0.7, size=n_samples)\n",
    "    return torch.tensor(m).to(device)\n",
    "\n",
    "# def sample_real(n, shape, std=1.):\n",
    "#     m = means_gaussian(std, n)\n",
    "#     return Variable(m, requires_grad=True)\n",
    "\n",
    "def sample_real(a=0.7, b=0.7, n_samples=1000):\n",
    "    #m = np.random.beta(a=0.7, b = 0.7, size=n_samples)\n",
    "    m = np.random.normal(0,1,size=n_samples)\n",
    "    return torch.tensor(m).type(torch.FloatTensor).to(device).unsqueeze(1).requires_grad_(True)\n",
    "\n",
    "\n",
    "def sample_noise(bs, d):\n",
    "    z = torch.rand(bs, d).type(torch.FloatTensor).to(device)\n",
    "    return z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "curdir = os.getcwd()\n",
    "mydir = os.path.join(curdir + \"/figs/\", \n",
    "                     datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + \"/\")\n",
    "#mydir=None\n",
    "print(mydir)\n",
    "if mydir != None:\n",
    "    try:\n",
    "        os.makedirs(mydir)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "            \n",
    "    with open(mydir + 'NetworkInfo.txt', 'w') as f:\n",
    "        f.write( 'generator: ' +  str(netG) + \"\\n\")\n",
    "        f.write('discriminator: ' +  str(netD) + '\\n')\n",
    "        if model == 'GAN-DP':\n",
    "            f.write('gamma: ' + str(gamma) + '\\n')\n",
    "        #f.write(\"\")\n",
    "        #f.write('normalization scale: ' +  norm_scale + '\\n')\n",
    "        #f.write('image height/width: ' + str(imageSize) + '\\n')\n",
    "        #f.write('loss function: BCE with Roth Penalty \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 2000\n",
    "n_latent = 1\n",
    "n_layers = 3\n",
    "n_hidden = 512\n",
    "bs = 256\n",
    "shape = 'beta'\n",
    "lamD = 10.\n",
    "extraD = 10\n",
    "model = 'GAN-DP'\n",
    "if model == 'WGAN-GP':\n",
    "    wgan = True\n",
    "else:\n",
    "    wgan=False\n",
    "gamma = 1.0\n",
    "verbose = True\n",
    "v_freq = 20\n",
    "n_samples = 5000\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    \n",
    "iteration = 0\n",
    "real_fwhms = []\n",
    "fake_fwhms = []\n",
    "real_means = []\n",
    "fake_means = []\n",
    "epochs = []\n",
    "\n",
    "\n",
    "netD = Perceptron([1] + [n_hidden] * n_layers + [1], final=None).to(device)\n",
    "netG = Perceptron([n_latent] + [n_hidden] * n_layers + [1]).to(device)\n",
    "print(netD)\n",
    "optD = torch.optim.Adam(netD.parameters(), lr=1e-3)\n",
    "optG = torch.optim.Adam(netG.parameters(), lr=1e-3)\n",
    "\n",
    "p_real = sample_real(a=0.7, b=0.7, n_samples=n_samples)\n",
    "p_nois = sample_noise(n_samples, n_latent)\n",
    "#print p_nois.data.numpy().shape\n",
    "#print p_real.data.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_wgan(fakeD, realD):\n",
    "    return fakeD.mean() - realD.mean()\n",
    "\n",
    "\n",
    "def objective_gan(fakeD, realD):\n",
    "    labD = torch.cat((torch.ones(fake.size(0), 1) - 1e-3,\n",
    "                      torch.zeros(real.size(0), 1) + 1e-3))\n",
    "    return bce(torch.cat((fakeD, realD)), (labD).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hist_width_mean(hist_data):\n",
    "\n",
    "    mean = hist_data.mean()\n",
    "    std = hist_data.std()\n",
    "    FWHM = 2.*np.sqrt(2.*np.log(2.))*std\n",
    "\n",
    "    return FWHM, mean\n",
    "\n",
    "\n",
    "\n",
    "def plot_all_metrics(real_FWHMs, fake_FWHMs, real_means, fake_means, epochs, n_epochs, save_dir =None, model='GAN'):\n",
    "\n",
    "    fig, (ax1,ax2) = plt.subplots(2,1,figsize=(4,8), sharey = False, sharex = True)\n",
    "\n",
    "    ax1.scatter(epochs, real_FWHMs, alpha = 0.75, label = 'real');\n",
    "    ax1.scatter(epochs, fake_FWHMs, alpha = 0.70, label = 'fake');\n",
    "    ax1.set_title(\"FWHMs\")\n",
    "    ax1.set_ylim(1, 4)\n",
    "    ax1.set_xlim(0,n_epochs)\n",
    "    #fig.suptitle(str(model) + \" Metrics\", x=0.5, y = 1.0, fontsize = 13) \n",
    "    ax1.text(750, 3.7, model, color='black', \n",
    "        bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "\n",
    "    ax2.scatter(epochs, real_means, alpha = 0.75, label = 'real');\n",
    "    ax2.scatter(epochs, fake_means, alpha = 0.70, label = 'fake');\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_title(\"Means\")\n",
    "    ax2.set_ylim(-1, 1)\n",
    "    ax2.set_xlim(0,n_epochs)\n",
    "    ax1.legend(loc='best')\n",
    "    ax1.legend(bbox_to_anchor=[1.3, -0.1], loc='center right')\n",
    "\n",
    "\n",
    "    if save_dir != None:\n",
    "        FWHMs_Means_comp = \"FHWMsMeans_Gaussian_Epoch\" + str(len(epochs)) + \".pdf\"\n",
    "        plt.savefig(save_dir + FWHMs_Means_comp, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_big(savedir=mydir, iteration = iteration):\n",
    "    p_nois = sample_noise(n_samples, n_latent)\n",
    "    outputs = netG(p_nois)\n",
    "    real_sample = sample_real(a=0.6, b=0.6,n_samples = 10000).cpu().data.numpy()\n",
    "    #print real_sample.shape\n",
    "    #print outputs.shape\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.hist(p_real.cpu().data.numpy(), alpha = 0.75, label = \"real\", bins=np.arange(-3.5,  3.5, 0.2))\n",
    "    ax.hist(outputs.cpu().data.numpy(), alpha = 0.75, label = \"fake\", bins=np.arange(-3.5, 3.5, 0.2))\n",
    "    ax.set_xlim(-4,4)\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title( \"Gaussian Generation, \" + str(model))\n",
    "    ax.yaxis.tick_left()\n",
    "    ax.yaxis.set_label_position(\"left\")\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    plt.text(-3.8, 355, 'mean: ' + str(round(outputs.cpu().data.numpy().mean(),3)), fontsize = 12, alpha = 0.8, color = 'xkcd:orange')\n",
    "    plt.text(-3.8, 335, 'std: ' + str(round(outputs.cpu().data.numpy().std(), 3)), fontsize = 12, alpha = 0.8, color = 'xkcd:orange')\n",
    "    plt.text(-3.8, 290, 'real mean: 0.0', fontsize = 12, alpha = 0.85, color = 'xkcd:blue')\n",
    "    plt.text(-3.8, 270, 'real std: ' + str(round(real_sample.std(),3)), fontsize = 12, alpha = 0.85, color = 'xkcd:blue')\n",
    "    plt.tight_layout()\n",
    "    if savedir != None:\n",
    "        plt.savefig(savedir + \"/GeneratedGaussianSample_LinearReluPerceptronGAN\" + str(n_layers) + \"Layers_\" + str(n_hidden) + \n",
    "                    \"hiddenPerLayer_size\" + str(bs) + \"Batch_\" + str(iteration) + \"Epoch_.png\")\n",
    "    return\n",
    "#plot_big()\n",
    "#itera+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "objective = objective_wgan if wgan else objective_gan\n",
    "\n",
    "iter_range = range(n_iterations)\n",
    "\n",
    "#print out time information if verbose is True\n",
    "#if verbose:\n",
    "#    iter_range = tqdm(iter_range)\n",
    "\n",
    "real_fwhms = []\n",
    "fake_fwhms = []\n",
    "real_means = []\n",
    "fake_means = []\n",
    "epochs = []\n",
    "#gamma= 1.0\n",
    "for iteration in iter_range:\n",
    "    if iteration % 4 == 0:\n",
    "        real = sample_real(bs, shape)\n",
    "        fake = netG(sample_noise(bs, n_latent))\n",
    "        fake_FWHM, fake_mean = hist_width_mean(fake.cpu().data.numpy())\n",
    "        real_FWHM, real_mean = hist_width_mean(real.cpu().data.numpy())\n",
    "        real_fwhms.append(real_FWHM)\n",
    "        fake_fwhms.append(fake_FWHM)\n",
    "        real_means.append(real_mean)\n",
    "        fake_means.append(fake_mean)\n",
    "        epochs.append(iteration)\n",
    "\n",
    "    \n",
    "    if iteration % 20 == 0:\n",
    "        plot_all_metrics(real_fwhms, fake_fwhms, real_means, fake_means, \n",
    "                        epochs, n_iterations, save_dir = mydir, model=model)\n",
    "        plot_big(savedir=mydir, iteration=iteration)\n",
    "    \n",
    "    for extra in range(extraD):\n",
    "        \n",
    "        # create a sample of real data in the shape of interest\n",
    "        netD.zero_grad()\n",
    "        real = sample_real(bs, shape)\n",
    "        fake = netG(sample_noise(bs, n_latent))\n",
    "        data = torch.cat((fake, real), 0)\n",
    "        \n",
    "\n",
    "        optD.zero_grad()\n",
    "        d_real = netD(real)\n",
    "        d_fake = netD(fake)\n",
    "        lossD = objective(d_real, d_fake)\n",
    "        if model == 'WGAN-GP':\n",
    "            #improved training of wasserstein gans\n",
    "            gradD = grad(lossD * bs, fake, create_graph=True)[0]\n",
    "            reguD = gradD.norm(2, 1).clamp(1).mean()\n",
    "            (lossD + lamD * reguD).backward()\n",
    "        \n",
    "        elif model == 'GAN-DP':\n",
    "            # Roth penalty\n",
    "            penalty = grad(d_real.sum(), real, create_graph=True)[0].view(-1,1).norm(2,1).pow(2).mean()\n",
    "            (lossD + (gamma/2) * penalty).backward()\n",
    "        \n",
    "        optD.step()\n",
    "\n",
    "    real = sample_real(bs, shape)\n",
    "    fake = netG(sample_noise(bs, n_latent))\n",
    "\n",
    "    optG.zero_grad()\n",
    "    lossG = - objective(netD(real), netD(fake))\n",
    "    (lossG).backward()\n",
    "    optG.step()\n",
    "    \n",
    "\n",
    "\n",
    "    if (iteration % v_freq) == 0:\n",
    "        #print \"Iteration: \" iteration, \"Discriminative Loss: \" lossD.data[0], \n",
    "              #\"Generative Loss: \" lossG.data[0], reguD.data[0]\n",
    "        print(\"Epoch #{}: Generative Loss: {}, Discriminative Loss: {}\".format(iteration + 1, lossG.data[0], lossD.data[0]))\n",
    "\n",
    "\n",
    "\n",
    "#Gloss, Dloss = train(netG, netD, shape = shape, wgan=False, verbose = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics(real_fwhms, fake_fwhms, real_means, fake_means, \n",
    "                        epochs, n_iterations, save_dir = mydir, model=model)\n",
    "itera = 1\n",
    "def plot_big(savedir=mydir,iteration = iteration):\n",
    "    p_nois = sample_noise(n_samples, n_latent)\n",
    "    outputs = netG(p_nois)\n",
    "    real_sample = sample_real(a=0.6, b=0.6,n_samples = 10000).cpu().data.numpy()\n",
    "    #print real_sample.shape\n",
    "    #print outputs.shape\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.hist(p_real.cpu().data.numpy(), alpha = 0.75, label = \"real\", bins=np.arange(-3.5,  3.5, 0.2))\n",
    "    ax.hist(outputs.cpu().data.numpy(), alpha = 0.75, label = \"fake\", bins=np.arange(-3.5, 3.5, 0.2))\n",
    "    ax.set_xlim(-4,4)\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title( \"Gaussian Generation, \" + str(model))\n",
    "    ax.yaxis.tick_left()\n",
    "    ax.yaxis.set_label_position(\"left\")\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    plt.text(-3.8, 355, 'mean: ' + str(round(outputs.cpu().data.numpy().mean(),3)), fontsize = 12, alpha = 0.8, color = 'xkcd:orange')\n",
    "    plt.text(-3.8, 335, 'std: ' + str(round(outputs.cpu().data.numpy().std(), 3)), fontsize = 12, alpha = 0.8, color = 'xkcd:orange')\n",
    "    plt.text(-3.8, 290, 'real mean: 0.0', fontsize = 12, alpha = 0.85, color = 'xkcd:blue')\n",
    "    plt.text(-3.8, 270, 'real std: ' + str(round(real_sample.std(),3)), fontsize = 12, alpha = 0.85, color = 'xkcd:blue')\n",
    "    plt.tight_layout()\n",
    "    if savedir != None:\n",
    "        plt.savefig(savedir + \"/GeneratedGaussianSample_LinearReluPerceptronGAN\" + str(n_layers) + \"Layers_\" + str(n_hidden) + \n",
    "                    \"hiddenPerLayer_size\" + str(bs) + \"Batch_\" + str(n_iterations) + \"Epochs_.pdf\")\n",
    "    return\n",
    "plot_big()\n",
    "itera+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(outputs, sample_real(1000,shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_nois = sample_noise(10000, n_latent)\n",
    "outputs = netG(p_nois).cpu().data.numpy()\n",
    "real_sample = sample_real(a=0.6, b=0.6,n_samples = 10000).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_histo, obins = np.histogram(outputs, bins = 20) \n",
    "real_sample_histo, sbins = np.histogram(real_sample, bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_probs = outputs_histo.astype(float) / outputs_histo.sum()\n",
    "real_probs = real_sample_histo.astype(float) / real_sample_histo.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(gen_probs, real_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "entro = 0\n",
    "for i in range(100):\n",
    "    p_nois = sample_noise(10000, n_latent)\n",
    "    outputs = netG(p_nois).cpu().data.numpy()\n",
    "    real_sample = sample_real(a=0.6, b=0.6,n_samples = 10000).cpu().data.numpy()\n",
    "    outputs_histo, obins = np.histogram(outputs, bins = 20) \n",
    "    real_sample_histo, sbins = np.histogram(real_sample, bins = 20)\n",
    "    gen_probs = (outputs_histo.astype(float) / outputs_histo.sum()) + .00001\n",
    "    real_probs = (real_sample_histo.astype(float) / real_sample_histo.sum()) + .00001\n",
    "    entro += entropy(gen_probs, real_probs)\n",
    "    \n",
    "entro = entro / 100\n",
    "print entro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
